{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From from this article: [Time Series â€” ARIMA vs. SARIMA vs. LSTM: Hands-On Tutorial](https://towardsdatascience.com/time-series-arima-vs-sarima-vs-lstm-hands-on-tutorial-bd5630298da3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to start off with downloading the data. It can be had from [here](https://archive.ics.uci.edu/dataset/360/air+quality). It's around 1 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Fetch dataset\n",
    "air_quality = fetch_ucirepo(id=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the data set.\n",
    "\n",
    "Same thing with many names:\n",
    "- Features, X, input variables, independent variables\n",
    "- Targets, y, output variables, dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data (as pandas dataframe)\n",
    "air_quality_df = air_quality.data.features\n",
    "air_quality_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation for this data: Missing values are tagged with -200 value. Let's\n",
    "see how many are -200 in each column. This can give us an idea of how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an element wise comparison on all values, then sum up for each column.\n",
    "missing_values_count = (air_quality_df == -200).sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great! We see how many values are effectively null. Let's get a percentage too\n",
    "so we can see for each column what the percentage of null (-200) there is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column. Rount to whole int\n",
    "missing_values_percentage = ((missing_values_count / len(air_quality_df)) * 100).round().astype(int)\n",
    "missing_values_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--TODO on second pass: For the readings, let's use the average value of the column to fill in the missing values. -->\n",
    "## Feature Engineering\n",
    "We'll handle missing values by dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column count and row count for before and after dropping missing values\n",
    "air_quality_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values and drop unnecessary columns.\n",
    "\n",
    "# Replace -200 with NaN\n",
    "air_quality_df.replace(-200, np.nan, inplace=True)\n",
    "# Drop columns where all values are NaN\n",
    "# - axis=1 means columns, how='all' means only drop if all values in the column are NaN\n",
    "air_quality_df.dropna(axis=1, how='all', inplace=True)\n",
    "air_quality_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with missing values.\n",
    "air_quality_df.dropna(inplace=True)\n",
    "air_quality_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, we lost most the data set. I don't like how the tutorial handled this.\n",
    "I think we can do better. TODO: come back to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the Date and Time columns into a single datetime column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Date and Time columns into a single datetime column\n",
    "air_quality_df['DateTime'] = pd.to_datetime(air_quality_df['Date'] + ' ' + air_quality_df['Time'])\n",
    "air_quality_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the new datetime column as the index since we're working with time series\n",
    "data, then sort the data by the index.\n",
    "Note ðŸ’¡: Inplace is more memory efficient than having it return one and then re-assigning\n",
    "it back to the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_df.set_index('DateTime', inplace=True)\n",
    "air_quality_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial picks Nox(GT) as the target variable without a reason why, so we'll use\n",
    "that too, but don't ask me why. ðŸ˜…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the target column.\n",
    "# Note ðŸ’¡: This also grabs the index column, so you'll have a df with two columns.\n",
    "target = air_quality_df['NOx(GT)']\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Let's visualize the data to see what we're working with. Visualizing is great. Always\n",
    "visualize. âœ¨ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(target)\n",
    "plt.title('Hourly NOx(GT) Levels')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('NOx(GT) Concentration')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "80/20 split. We'll use the last 20% of the data as the test set.\n",
    "Note ðŸ’¡: When the index for the data is an integer we can do normal slicing. But when\n",
    "the index is an object like a date, which is what we have here, we need to use iloc\n",
    "instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index for where 80% falls.\n",
    "train_size = int(len(target) * 0.8)\n",
    "\n",
    "\n",
    "train, test = target.iloc[:train_size], target.iloc[train_size:]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Now to the fun part! We can start digging into making models.\n",
    "\n",
    "Some definitions:\n",
    "- Stationary: Mean, variance, and autocorrelation do not change over time. Autocorrelation is a value compared to a previous value. Often referred to as the lag. The lag is a time step and how many of that time step we're going to compare a given value to in the past. A good example of a stationary data set would be the temperature in a room set by a thermostat. The thermostat is set to something like 74ËšF and the temperature will fluctuate around that value. The mean, variance, and autocorrelation will not change over time.\n",
    "- Non-Stationary: Mean, variance, and autocorrelation do change over time. When they change, there's some specific terms for the ways they change.\n",
    "  - Trend: The average changes over time. For example the cost of living goes up overtime because of inflation.\n",
    "  - Seasonality: The variance changes over time, usually on a regular cadence. Like sales that are affected by the time of year like around holidays and the like.\n",
    "  - Cyclical: The autocorrelation changes over time. This is a repeating pattern that occurs at irregular intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make non-stationary data into stationary data. There's some ways to do this, we'll use differencing here. Differencing is subtracting the previous value from the current value. This aims to remove trends from the data.\n",
    "\n",
    "Some ways exist to tell if a dataset needs differencing. We'll use ADF (Augmented Dickey-Fuller) test. This test looks for Mean Reversion vs. Random Walk. In a stationary series, values may fluctuate, but revert to ta stable mean. In a non-stationary series, it will act more like someone walking at random where the next step doesn't depend on the previous step.\n",
    "With this, if the p-value is less than a threshold, say 5%, then the dataset is stationary. If it's greater than 5%, then the dataset is non-stationary and we should difference it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# ADF Test\n",
    "adf_result = adfuller(train)\n",
    "\n",
    "# Pull the p-value and make it have 1 decimal place\n",
    "p_value = (adf_result[1]*100).round(1)\n",
    "\n",
    "print(f'P-Value: {p_value}%' + (' (Stationary)' if p_value < 5 else ' (Non-Stationary)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now difference the data. Each time we difference it, it is one order of\n",
    "differencing. So this is a first order differencing. You can difference it more but\n",
    "doing it more than twice, 3rd order differencing, and higher generally ends up hurting\n",
    "more than helping and ruins the data. So we'll do it once, maybe twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-order differencing\n",
    "train_diff = train.diff()\n",
    "\n",
    "# Remove the first NaN value\n",
    "train_diff.dropna(inplace=True)\n",
    "train_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check ADF p-value again\n",
    "adf_result = adfuller(train_diff)\n",
    "p_value = (adf_result[1]*100).round(1)\n",
    "\n",
    "print(f'P-Value: {p_value}%' + (' (Stationary)' if p_value < 5 else ' (Non-Stationary)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The data is now stationary. We can now move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive Integrated Moving Average. ARIMA is a univariate linear model. It does not support seasonality in data. ARIMA parameters are: ARIMA(p, d, q). These three parameters account for seasonality, p, trend, d, and noise, q, in data.\n",
    "  - p: It indicates the number of past values (lags) of the time series that the model will use to predict the current value. p=1 means that the model will use the last value to predict the current value, whicle p=2 means that the model will use the last two values to predict the current value, and so on. These are the order of the autoregressive, AR, part of the model.\n",
    "  - d: It indicates the number of differences that the model will use to make the time series stationary. d=1 means that the model will use the first difference of the time series, d=2 means that the model will use the second difference of the time series, and so on. The order of integration, I, part of the model.\n",
    "  - q: The number of previous forecast errors to use to adjust the current forecast prediction. The moving average, MA, part of the model. q=1 means that the model will use the last forecast error to adjust the current forecast, q=2 means that the model will use the last two forecast errors to adjust the current forecast, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Darts as our framework for all the models.\n",
    "\n",
    "It's good to know what the parameters are and how they are calculated once we're comfortable with that, we can just use the auto finders to find the best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.models import AutoARIMA\n",
    "\n",
    "# Convert the pandas series to a Darts TimeSeries\n",
    "darts_train = TimeSeries.from_series(train)\n",
    "\n",
    "# Create the model\n",
    "model = AutoARIMA(\n",
    "    start_p=1,\n",
    "    start_q=1,\n",
    "    max_p=5,\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

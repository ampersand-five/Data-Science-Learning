{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From from this article: [Time Series â€” ARIMA vs. SARIMA vs. LSTM: Hands-On Tutorial](https://towardsdatascience.com/time-series-arima-vs-sarima-vs-lstm-hands-on-tutorial-bd5630298da3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to start off with downloading the data. It can be had from [here](https://archive.ics.uci.edu/dataset/360/air+quality). It's around 1 MB.\n",
    "\n",
    "The data is a gas sensor array with hourly readings. The readings are averaged from 5\n",
    "sensors for each hourly entry. Columns with (GT) are the ground truth values from a\n",
    "co-located reference sensor, similarly averaged across multiple sensors, averaged\n",
    "hourly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset\n",
    "air_quality = fetch_ucirepo(id=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the data set.\n",
    "\n",
    "Same thing with many names:\n",
    "- Features, X, input variables, independent variables\n",
    "- Targets, y, output variables, dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data (as pandas dataframe)\n",
    "air_quality_df = air_quality.data.features\n",
    "air_quality_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the data a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See statistics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation for this data: Missing values are tagged with -200 value. Let's\n",
    "see how many are -200 in each column. This can give us an idea of how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an element wise comparison on all values, then sum up for each column.\n",
    "missing_values_count = (air_quality_df == -200).sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great! We see how many values are effectively null. Let's get a percentage too\n",
    "so we can see for each column what the percentage of null (-200) there is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column. Rount to whole int\n",
    "missing_values_percentage = ((missing_values_count / len(air_quality_df)) * 100).round().astype(int)\n",
    "missing_values_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--TODO on second pass: For the readings, let's use the average value of the column to fill in the missing values. -->\n",
    "## Feature Engineering\n",
    "We'll handle missing values by dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column count and row count for before and after dropping missing values\n",
    "air_quality_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values and drop unnecessary columns.\n",
    "\n",
    "# Replace -200 with NaN\n",
    "air_quality_df.replace(-200, np.nan, inplace=True)\n",
    "# Drop columns where all values are NaN\n",
    "# - axis=1 means columns, how='all' means only drop if all values in the column are NaN\n",
    "air_quality_df.dropna(axis=1, how='all', inplace=True)\n",
    "air_quality_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with missing values.\n",
    "air_quality_df.dropna(inplace=True)\n",
    "air_quality_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, we lost most the data set. I don't like how the tutorial handled this.\n",
    "I think we can do better. TODO: come back to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the Date and Time columns into a single datetime column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Date and Time columns into a single datetime column\n",
    "air_quality_df['DateTime'] = pd.to_datetime(air_quality_df['Date'] + ' ' + air_quality_df['Time'])\n",
    "air_quality_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the new datetime column as the index since we're working with time series\n",
    "data, then sort the data by the index.\n",
    "Note ðŸ’¡: Inplace is more memory efficient than having it return one and then re-assigning\n",
    "it back to the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_df.set_index('DateTime', inplace=True)\n",
    "air_quality_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial picks Nox(GT) as the target variable without a reason why, so we'll use\n",
    "that too, but don't ask me why. ðŸ˜…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the target column.\n",
    "# Note ðŸ’¡: This also grabs the index column, so you'll have a df with two columns.\n",
    "target = air_quality_df['NOx(GT)']\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "Let's visualize the data to see what we're working with. Visualizing is great. Always\n",
    "visualize. âœ¨ðŸ“Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(target)\n",
    "plt.title('Hourly NOx(GT) Levels')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('NOx(GT) Concentration')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "80/20 split. We'll use the last 20% of the data as the test set.\n",
    "Note ðŸ’¡: When the index for the data is an integer we can do normal slicing. But when\n",
    "the index is an object like a date, which is what we have here, we need to use iloc\n",
    "instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index for where 80% falls.\n",
    "train_size = int(len(target) * 0.8)\n",
    "\n",
    "\n",
    "train, test = target.iloc[:train_size], target.iloc[train_size:]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Now to the fun part! We can start digging into making models.\n",
    "\n",
    "Some definitions:\n",
    "- Autocorrelation: Measures the correlation of a time series with a lagged (previous) version of itself. It indicates how past values of the series are related to current values. If a time series of temperature readings shows high, the current is influenced by the past, like today's temperature is influenced by yesterday's temperature.\n",
    "\n",
    "- Stationary: Mean, variance, and autocorrelation do not change over time. A good example of a stationary data set would be the humidity in a greenhouse. The humidity will fluctuate around the set value. The mean, variance, and autocorrelation will not change over time.\n",
    "- Non-Stationary: Mean, variance, and autocorrelation do change over time. When they change, there's some specific terms for the ways they change.\n",
    "  - Trend: The changes in in mean over time. For example the cost of living goes up overtime because of inflation.\n",
    "  - Seasonality: Changes on a regular cadence. Like sales that are affected by the time of year like around holidays.\n",
    "  - Variation: How the spread of the data changes over time. Measure of fluctuation around the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make non-stationary data into stationary data. There's some ways to do this, we'll use differencing here. Differencing is subtracting the previous value from the current value. This aims to remove trends from the data.\n",
    "\n",
    "Some ways exist to tell if a dataset needs differencing. We'll use ADF (Augmented Dickey-Fuller) test. This test looks for Mean Reversion vs. Random Walk. In a stationary series, values may fluctuate, but revert to a stable mean. In a non-stationary series, it will act more like someone walking at random where the next step doesn't depend on the previous step.\n",
    "With this, if the p-value is less than a threshold, say 5%, then the dataset is stationary. If it's greater than 5%, then the dataset is non-stationary and we should difference it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# ADF Test\n",
    "adf_result = adfuller(train)\n",
    "\n",
    "# Grab the p-value and make it have 1 decimal place\n",
    "p_value = (adf_result[1]*100).round(1)\n",
    "\n",
    "print(f'P-Value: {p_value}%' + (' (Stationary)' if p_value < 5 else ' (Non-Stationary)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's greater than 5% that we want to see, so we'll difference the data.\n",
    "\n",
    "Each time we difference it, it is one order of differencing. So this is a first order\n",
    "differencing. You can difference it more but doing it more than twice, ends up hurting\n",
    "more than helping and ruins the data. So we'll do it once, maybe twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-order differencing\n",
    "train_diff = train.diff()\n",
    "\n",
    "# Remove the first NaN value\n",
    "train_diff.dropna(inplace=True)\n",
    "train_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check ADF p-value again\n",
    "adf_result = adfuller(train_diff)\n",
    "p_value = (adf_result[1]*100).round(1)\n",
    "\n",
    "print(f'P-Value: {p_value}%' + (' (Stationary)' if p_value < 5 else ' (Non-Stationary)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The data is now stationary. Now that we understand that better, let's hand it off\n",
    "to an auto finder that can find the best parameters for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive Integrated Moving Average. ARIMA is a univariate linear model. It does not support seasonality in data. ARIMA parameters are: ARIMA(p, d, q). These three parameters account for seasonality, p, trend, d, and noise, q, in data.\n",
    "  - p: Indicates the number of past values (lags) of the time series that the model will use to predict the current value. p=1 means that the model will use the last value to predict the current value, whicle p=2 means that the model will use the last two values to predict the current value, and so on. This is the order of the autoregressive, AR, part of the model.\n",
    "  - d: Indicates the number of differences that the model will use to make the time series stationary. d=1 means that the model will use the first difference of the time series, d=2 means that the model will use the second difference of the time series, and so on. This is the order of integration, I, part of the model.\n",
    "  - q: The number of previous forecast errors to use to adjust the current forecast prediction. The moving average, MA, part of the model. q=1 means that the model will use the last forecast error to adjust the current forecast, q=2 means that the model will use the last two forecast errors to adjust the current forecast, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use Darts as our framework for all the models.\n",
    "\n",
    "It's good to know what the parameters are and how they are calculated. Once we're comfortable with that, we can just use the auto finders to find the best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "# This implementation is a thin wrapper around pmdarima AutoARIMA model\n",
    "from darts.models import AutoARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of note is that Darts will not be able to infer the frequency of this data. Sometimes it\n",
    "can, but with this data it can't. We can check this by looking to see if\n",
    "`train.index.freq` is None. If it is, the frequency is not set. This data is hourly, so\n",
    "we can set the frequency to 'h'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.index)\n",
    "# If train.index.freq is None, then we need to set it manually\n",
    "print(train.index.freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has some gaps in the readings, not every hour has a reading. When we set the\n",
    "frequency to 'h', it will fill in the missing values with NaN by default, but we can't\n",
    "use NaN's in the model. We'll use forward fill for now just to input something. TODO:\n",
    "do something smarter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the frequency to hourly\n",
    "train = train.asfreq('h', method='ffill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check NaN counts\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the pandas series to a Darts TimeSeries\n",
    "darts_train = TimeSeries.from_series(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the model\n",
    "arima_model = AutoARIMA(\n",
    "    darts_train,\n",
    "    start_p=1,\n",
    "    max_p=24, # 24 hours in a day, so if there's a cycle it can see this\n",
    "    #d=None, # let model determine 'd' - None is the default\n",
    "    start_q=1,\n",
    "    max_q=24, # 24 hours in a day, so if there's a cycle it can see this\n",
    "    # m=1, # Frequency of series, 1 means a non-seasonal series. 4 would be quarterly,\n",
    "    #   etc. Default is 1.\n",
    "    # seasonal=False, # Non-seasonal data. Default True, but if m=1, this is set to\n",
    "    #   False. It can be left commented out because m=1 is the default.\n",
    "    test='adf', # Test to determine d. Default is kpss.\n",
    "    # stepwise=True, # Method for determining the best parameters. When using stepwise\n",
    "    #   it can fit faster and be less likely to over-fit. Default is True.\n",
    "    trace=True # Print some debug info on fits. Default is False.\n",
    "    # suppress_warnings=True # Set to True to suppress warnings from statsmodels.\n",
    "    #   Default is True.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we instantiate the ARIMA model with the data it calls fit, so we can check the\n",
    "summary of the model right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model.fit(darts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model.model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
